{
	"comments":[
		"Is each field name exotically long as well?\r\n\r\nLucene used to do just this -- intern {{FieldInfo.name}} and then use `==` to compare field names everywhere.  But we decided long ago that this was dangerous and not an important optimization.  Still, that decision was maybe pre Java 7 days when the intern'd pool was stored in {{PermGen}} instead of \"ordinary\" heap and was more likely to cause {{OutOfMemoryError}}?\r\n\r\nMaybe dig into those long ago issues / dev list thread to see the motivation to stop interning?",
		"The field names in the particular case that lead to this issue were indeed a little longer than usual (about 100-150 chars each). But the problem of duplicate strings in the `FieldInfo` is not limited to the field names. Analyzing the heap dump that motivated this issue I found the biggest contributors to duplicate strings as follows:\r\n\r\nÂ \r\n\r\n!image-2022-08-08-13-23-37-050.png!\r\n\r\nTo vastly improve the situation, we wouldn't even need to look into interning field names (though that would still be nice and a GB scale win in this case as well). If we were to just intern or deduplicate the obvious things like \"PerFieldPostingsFormat.format\" or \"Lucene80\" that we already have in the string constant pool anyway that would offer a trivial win for cases like this one.\r\n\r\nPS: Created https://issues.apache.org/jira/browse/LUCENE-10677 as a separate issue for the non-field-name strings.",
		"I'm opposed to use of String.intern here.\r\n\r\nThe problem here is the user, they have 10,000 fields as you described. That's completely unnecessary.",
		"This issue was moved to GitHub issue: [#11711|https://github.com/apache/lucene/issues/11711]."
	],
	"description":["We encountered an Elasticsearch user with high heap usage, a significant proportion of which was down to the contents of `FieldInfo#name`.\r\n\r\nThis user was certainly pushing some scalability boundaries: this single process had thousands of active Lucene indices, many with 10k+ fields, and many indices had hundreds of segments due to an excess of flushes, so in total they had an enormous number of `FieldInfo` instances. Still, the bulk of the heap usage was just field names, and the total number of distinct field names was fairly small. That's pretty common, especially for time-based data like logs. Some kind of interning or deduplication of these strings would have reduced their heap usage by many GBs.\r\n\r\nIs there a way we could deduplicate these strings? Deduplicating them across segments within each index would already have helped, but ideally we'd like to deduplicate them across indices too."],
	"summary":["FieldInfo#name contributes significantly to heap usage at scale"]
}